# SMS Spam Classifier (NLP â€“ Easy Project)

A clean, reproducible **spam vs ham** SMS classifier that showcases endâ€‘toâ€‘end NLP skills recruiters care about: EDA, modeling, evaluation, interpretability, and reproducibility.

## ğŸ” Overview
**Task:** Binary text classification (spam vs ham) on the classic _SMS Spam Collection_ dataset (5,574 messages).  
**Approach:** `TFâ€‘IDF` features over unigrams + bigrams â†’ `Logistic Regression` baseline.  
**Why this matters:** Strong, interpretable baseline; great canvas to discuss tradeâ€‘offs (precision vs recall), thresholds, and deployment.

<p align="center">
<img alt="ROC Curve" src="reports/figures/roc_test.png" width="55%">
<img alt="Confusion Matrix" src="reports/figures/confusion_matrix_test.png" width="43%">
</p>

## Project Structure
```
sms-spam-classifier/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/SMSSpamCollection            # raw dataset (TSV: label \t text)
â”‚   â””â”€â”€ processed/                       # (optional) train/val/test splits
â”œâ”€â”€ models/
â”‚   â””â”€â”€ latest/model.joblib              # trained pipeline (TF-IDF + LR)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_eval_plots.ipynb
â”‚   â””â”€â”€ 03_threshold_tuning.ipynb
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ figures/                         # exported plots
â”‚   â”œâ”€â”€ top_features_ham.txt             # most negative weights (ham)
â”‚   â””â”€â”€ top_features_spam.txt            # most positive weights (spam)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                         # train + evaluate + save model
â”‚   â”œâ”€â”€ infer.py                         # CLI scoring, prints label & p_spam
â”‚   â””â”€â”€ top_ngrams.py                    # prints top weighted n-grams
â”œâ”€â”€ environment.yml                      # conda environment (recommended)
â”œâ”€â”€ requirements.txt                     # pip alternative
â””â”€â”€ README.md
```

## Quickstart
```bash
# 0) Create & activate isolated environment
conda env create -f environment.yml
conda activate nlp-easy
# or: python -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt

# 1) Put the data in place (TSV, no header)
#    Path should be: data/raw/SMSSpamCollection

# 2) Train & evaluate (saves models/latest/model.joblib)
python src/train.py --data data/raw/SMSSpamCollection

# 3) Inspect top features
python src/top_ngrams.py --model_dir models/latest --k 25

# 4) Score new messages
python src/infer.py --model_dir models/latest --text "Free entry to win Â£1000! Reply STOP to opt-out"
printf '%s\n%s\n' 'win cash now!!!' 'Hey, are we meeting at 5?' | python src/infer.py --model_dir models/latest
```

## Results (reproducible baseline)
On a heldâ€‘out split (15% val, 15% test, stratified; random_state=42):

| Split | Accuracy | Spam Precision | Spam Recall | F1 (spam) | ROCâ€‘AUC |
|---|---:|---:|---:|---:|---:|
| **Validation** | 0.968 | 1.000 | 0.759 | 0.863 | **0.996** |
| **Test**       | 0.962 | 1.000 | 0.714 | 0.833 | **0.986** |

**Interpretation:** The baseline is highly precise for spam (few false positives) but misses some spam (recall ~0.71). Use **threshold tuning** (Notebook `03_threshold_tuning.ipynb`) to trade precision for recall as needed.

## Plots & Analysis
- **ROC curve** and **Confusion matrix** â†’ run `02_eval_plots.ipynb`.
- **Threshold sweep (val)** â†’ run `03_threshold_tuning.ipynb` for precision/recall/F1 curves and a suggested threshold.
- **Top weighted nâ€‘grams** â†’ `python src/top_ngrams.py` (writes into `reports/`).

## Interactive Notebooks (HTML Exports)

- [01_eda.html](notebooks/exports/01_eda.html)
- [02_eval_plots.html](notebooks/exports/02_eval_plots.html)
- [03_threshold_tuning.html](notebooks/exports/03_threshold_tuning.html)

## Model Card (Short)
**Intended use.** Educational demo for SMS spam detection; not productionâ€‘hardened.  
**Data.** SMS Spam Collection v1 (English; UK/Singapore origin). One message per line: `label \t text` (labels: `ham`, `spam`).  
**Preprocessing.** Lowercasing, accent stripping; tokenization via `TfidfVectorizer` with `ngram_range=(1,2)`, `min_df=2`, `max_df=0.95`. No stemming/lemmatization.  
**Model.** Logistic Regression (scikitâ€‘learn) on TFâ€‘IDF features; outputs `p(spam)`; default threshold `0.5`.  
**Metrics.** Accuracy, precision, recall, F1 (classâ€‘wise), ROCâ€‘AUC; confusion matrix on heldâ€‘out test set.  
**Limitations.** Domain/temporal bias in dataset; limited to English SMS; might miss obfuscated/modern spam patterns.  
**Responsible use.** Calibrate threshold to your tolerance for false positives/negatives; monitor drift; consider humanâ€‘inâ€‘theâ€‘loop review in production.

## Reproducibility
- Deterministic split with `random_state=42`.  
- Frozen environment via `environment.yml` / `requirements.txt`.  
- All figures and artifacts saved in `reports/` and `models/`.

## Roadmap / Extensions
- DistilBERT baseline comparison (fewâ€‘epoch fineâ€‘tune).  
- Add **--threshold** flag to `infer.py`.  
- Simple **FastAPI** endpoint for scoring.  
- Adversarial spam patterns / robustness checks.

---

**Author:** Nursultan Azhimuratov ([@drnursultan](https://github.com/drnursultan))  
If you use this repo, a star â­ï¸ or a note is appreciated!
